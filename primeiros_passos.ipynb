{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "name": "primeiros_passos",
    "notebookId": 3892328798844911,
    "colab": {
      "name": "primeiros_passos.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3eqkIQOEDz6"
      },
      "source": [
        "# Primeiros passos com PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNnPdkuHED0O"
      },
      "source": [
        "Neste Objeto de Aprendizagem daremos nossos primeiros passos com o PySpark e Spark Dataframes. O objetivo aqui é conhecer os principais objetos do PySpark e introduzir os métodos mais básicos para familiarizar com a tecnologia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsLWL9bdED0S"
      },
      "source": [
        "Começaremos pela importação do pacote do PySpark que engloba as operações com DataFrames e então criaremos um pequeno DataFrame que será utilizado nos exemplos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR_Lm-dqED0U"
      },
      "source": [
        "### Bibliotecas necessárias\n",
        "\n",
        "Por enquanto precisaremos somente do módulo `pyspark.sql`. O pacote PySpark possui diversos módulos, mas por enquanto precisaremos somente dos objetos que estão em `pyspark.sql`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iikR-80tERzw"
      },
      "source": [
        "# instalar as dependências\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# configurar as variáveis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "# tornar o pyspark \"importável\"\n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JITuELPPED0X"
      },
      "source": [
        "# Uso do Spark Dataframes no PySpark\n",
        "from pyspark.sql import *"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_ZFRiXFED0e"
      },
      "source": [
        "### Conectando com o Spark\n",
        "\n",
        "O próximo passo é iniciar uma sessão do Spark (`SparkSession`), cujo papel é o de comunicação com o Cluster. No exemplo abaixo criaremos uma sessão local - ou seja, com um minicluster na sua própria máquina. Esta sessão local é definida por meio do método `master`. O método `master` indica qual o tipo de Cluster onde conectaremos e outros detalhes. \n",
        "\n",
        "No nosso caso indicamos que o tipo de Cluster é local e que este utilizaremos 2 processadores para execução das tarefas do Spark. Percebam que mesmo em modo local temos à nossa disposição a capacidade de processamento _multicore_.\n",
        "\n",
        "**IMPORTANTE**: No ambiente da **Databricks** não precisamos criar uma Sessão pois o notebook será vinculado (_attached_) a um cluster no momento da execução. Logo, o bloco de código abaixo não será necessário **na Databricks**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcpRqsVtED0h"
      },
      "source": [
        "# Vamos trabalhar com o Spark localmente, sem o uso de um cluster.\n",
        "#spark = SparkSession \\\n",
        "#    .builder \\\n",
        "#    .master(\"local[2]\") \\\n",
        "#    .appName(\"Primeiros passos\") \\\n",
        "#    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4vGQ_0CED0k"
      },
      "source": [
        "## Criação de um Data Frame via código\n",
        "\n",
        "Nesta sessão criaremos um DataFrame diretamente via código. Esta não é uma prática muito comum, visto que trabalhamos com grandes volumes de dados obtidos através de Sistemas de Armazenamento. \n",
        "\n",
        "Mas não pensem que a criação de um DataFrame via código só serve para exemplos! Ainda veremos casos onde esta prática ajudará na resolução de problemas!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kpD9HtRED0o"
      },
      "source": [
        "### Exemplo Prático\n",
        "\n",
        "Nosso exemplo prático utiliza a relação de disciplinas que compõe esta especialização e suas cargas horárias!\n",
        "\n",
        "Os dados para este exemplo foram obtidos **manualmente** da página de Estrutura Curricular do curso, disponível [aqui](http://www.unisinos.br/especializacao/big-data-data-science-e-data-analytics/ead/sao-leopoldo/estrutura-curricular)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RyUNAeiED0r"
      },
      "source": [
        "#### Definição da estrutura: Registros e Colunas\n",
        "\n",
        "Para criar um DataFrame por código precisamos inicialmente definir sua estrutura. A linha de código abaixo define que nosso DataFrame será formado por disciplinas, onde cada registro (**Row**) será uma disciplina. Os atributos de uma disciplina disponíveis serão o _nome_ e a _carga horária_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK2v7HsTED0u"
      },
      "source": [
        "# Estrutura do nosso DataFrame\n",
        "Disciplina = Row(\"nome\", \"carga_horaria\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi7eeyvaED0w"
      },
      "source": [
        "#### Criação de instâncias (registros e atributos)\n",
        "\n",
        "Nosso próximo passo é a criação de instâncias para popular o DataFrame. Usaremos a estrutura `Disciplina` recém criada para instanciar cada uma das disciplinas da especialização e sua carga horária. \n",
        "\n",
        "Neste exemplo foi criada uma referência (`d01` a `d14`) por disciplina para deixar o código mais claro. No passo seguinte criaremos uma lista que agrupará todas as disciplinas e servirá de fonte para envio dos dados ao Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnwawBmhED0y"
      },
      "source": [
        "# Cada uma das disciplinas da especialização é criada como uma instância do registro Disciplina.\n",
        "\n",
        "d01 = Disciplina(\"Introdução a BigData e Analytics\", 36)\n",
        "d02 = Disciplina(\"Estatística aplicada\", 24)\n",
        "d03 = Disciplina(\"Visualização de dados e informação\", 24)\n",
        "d04 = Disciplina(\"Compartilhamento e segurança de dados\", 24)\n",
        "d05 = Disciplina(\"Introdução a Python e linguagem R\", 36)\n",
        "d06 = Disciplina(\"Machine Learning\", 24)\n",
        "d07 = Disciplina(\"Processamento de Alto Desempenho e Aplicações\", 24)\n",
        "d08 = Disciplina(\"Lidando com BigData: Apache Spark, Hadoop, MapReduce, Hive\", 24)\n",
        "d09 = Disciplina(\"Gerenciamento e Processamento de grande volume de dados\", 24)\n",
        "d10 = Disciplina(\"Internet das Coisas e Aplicações Distribuídas\", 24)\n",
        "d11 = Disciplina(\"Deep Learning\", 24)\n",
        "d12 = Disciplina(\"Business Intelligence e BigData\", 24)\n",
        "d13 = Disciplina(\"Atividades Integradoras\", 12)\n",
        "d14 = Disciplina(\"Preparação para Projeto Aplicado\", 36)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYQ3u5fxED00"
      },
      "source": [
        "Por meio da função `display` temos uma prévia do que será nosso DataFrame! Atentem para o fato de que até aqui nossos dados estão no Python e não no Spark. Ainda não temos um DataFrame!\n",
        "\n",
        "No ambiente **Databricks** o resultado da função `display` será apresentado de forma mais amigável pois o mecanismo de notebooks do ambiente está preparado para formatação dos objetos Row do Spark. Lá a visualização da lista `Row` e posteriormente do DataFrame serão muito parecidas!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpm2mZ56ED01",
        "outputId": "869a42e7-0608-48f1-9c49-3f3c1b6d296f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "especializacao_bigdata_datascience = [d01, d02, d03, d04, d05, d06, d07, d08, d09, d10, d11, d12, d13, d14]\n",
        "\n",
        "display(especializacao_bigdata_datascience)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[Row(nome='Introdução a BigData e Analytics', carga_horaria=36),\n",
              " Row(nome='Estatística aplicada', carga_horaria=24),\n",
              " Row(nome='Visualização de dados e informação', carga_horaria=24),\n",
              " Row(nome='Compartilhamento e segurança de dados', carga_horaria=24),\n",
              " Row(nome='Introdução a Python e linguagem R', carga_horaria=36),\n",
              " Row(nome='Machine Learning', carga_horaria=24),\n",
              " Row(nome='Processamento de Alto Desempenho e Aplicações', carga_horaria=24),\n",
              " Row(nome='Lidando com BigData: Apache Spark, Hadoop, MapReduce, Hive', carga_horaria=24),\n",
              " Row(nome='Gerenciamento e Processamento de grande volume de dados', carga_horaria=24),\n",
              " Row(nome='Internet das Coisas e Aplicações Distribuídas', carga_horaria=24),\n",
              " Row(nome='Deep Learning', carga_horaria=24),\n",
              " Row(nome='Business Intelligence e BigData', carga_horaria=24),\n",
              " Row(nome='Atividades Integradoras', carga_horaria=12),\n",
              " Row(nome='Preparação para Projeto Aplicado', carga_horaria=36)]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHarbS-RED03"
      },
      "source": [
        "#### Criação do DataFrame por meio da transferência dos dados da lista\n",
        "\n",
        "Lembram que mais acima eu descrevi o `SparkSession` como o canal de comunicação com o Cluster? Pois bem, agora veremos na prática o que isso significa. Nossa sessão possibilita a criação de um DataFrame pelo método `createDataFrame`. Este método:\n",
        "- envia a lista de objetos `Row` para o Cluster\n",
        "- cria uma estrutra de DataFrame no Cluster\n",
        "- popula o DataFrame com os objetos `Row` recebidos\n",
        "- retorna a referência ao DataFrame para o Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS6sz0C1ED04"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "df_especializacao = spark.createDataFrame(especializacao_bigdata_datascience)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g75NP5ETED06"
      },
      "source": [
        "Voltaremos a usar a função `display`, desta vez para inspecionar o conteúdo da referência ao DataFrame que o método `createDataFrame` retornou para nós. Aqui percebemos que se trata de um DataFrame, e que ele possui duas colunas:\n",
        "\n",
        "- nome: string\n",
        "- carga_horaria: bigint\n",
        "\n",
        "**Importante**: Na **Databricks** a função `display` exibe o conteúdo do nosso DataFrame de forma bastante similar a quando usamos `display` para visualizar o conteúdo da lista de objetos `Row`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3eNAFT4ED07",
        "outputId": "a37b85f2-8a0f-4efd-c739-daa4d6b957da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "display(df_especializacao)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[nome: string, carga_horaria: bigint]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ge-a1LTED08"
      },
      "source": [
        "### Visualização de dados de um DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sof18u84ED09"
      },
      "source": [
        "#### Método `show`\n",
        "\n",
        "O método `show` exibe registros do DataFrame formatados em modo texto. Se a chamada ao método for sem nenhum parâmetro ele retornará uma tabela com os nomes das coluns em cabeçalho, registros até um máximo de 20 linhas e os valores das colunas de tipo String (texto) serão exibidos até um máximo de 20 caracteres.\n",
        "\n",
        "A documentação do método `show` ([link](http://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show)) detalha os seguintes parâmetros:\n",
        "\n",
        "- **n** – Número de registros a exibir. Se quisermos uma quantidade diferente de 20 registros então devemos informar a quantidade neste parâmetro.\n",
        "- **truncate** – Se 20 caracteres for pouco (e no nosso exemplo vimos que é pouco) então devemos informar quantos caracteres das colunas String devem ser mostrados. Se o DataFrame tiver muitas colunas do tipo String a visualização pode ficar difícil.\n",
        "- **vertical** – Se for False (padrão), exibe em formato de tabela. Se for True, exibirá cada coluna em uma linha, em formato de lista de valores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egTiOFZ_ED0_",
        "outputId": "464b5c15-858c-42ed-de45-571bf48a69d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_especializacao.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------+\n",
            "|                nome|carga_horaria|\n",
            "+--------------------+-------------+\n",
            "|Introdução a BigD...|           36|\n",
            "|Estatística aplicada|           24|\n",
            "|Visualização de d...|           24|\n",
            "|Compartilhamento ...|           24|\n",
            "|Introdução a Pyth...|           36|\n",
            "|    Machine Learning|           24|\n",
            "|Processamento de ...|           24|\n",
            "|Lidando com BigDa...|           24|\n",
            "|Gerenciamento e P...|           24|\n",
            "|Internet das Cois...|           24|\n",
            "|       Deep Learning|           24|\n",
            "|Business Intellig...|           24|\n",
            "|Atividades Integr...|           12|\n",
            "|Preparação para P...|           36|\n",
            "+--------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZEk7pToED1A",
        "outputId": "5ce454e4-22de-4991-f7ab-b85b337e5c0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Lista de registros, exibindo os primeiros 60 caracteres de cada nome.\n",
        "df_especializacao.show(vertical=True, truncate=60)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-RECORD 0-------------------------------------------------------------------\n",
            " nome          | Introdução a BigData e Analytics                           \n",
            " carga_horaria | 36                                                         \n",
            "-RECORD 1-------------------------------------------------------------------\n",
            " nome          | Estatística aplicada                                       \n",
            " carga_horaria | 24                                                         \n",
            "-RECORD 2-------------------------------------------------------------------\n",
            " nome          | Visualização de dados e informação                         \n",
            " carga_horaria | 24                                                         \n",
            "-RECORD 3-------------------------------------------------------------------\n",
            " nome          | Compartilhamento e segurança de dados                      \n",
            " carga_horaria | 24                                                         \n",
            "-RECORD 4-------------------------------------------------------------------\n",
            " nome          | Introdução a Python e linguagem R                          \n",
            " carga_horaria | 36                                                         \n",
            "-RECORD 5-------------------------------------------------------------------\n",
            " nome          | Machine Learning                                           \n",
            " carga_horaria | 24                                                         \n",
            "-RECORD 6-------------------------------------------------------------------\n",
            " nome          | Processamento de Alto Desempenho e Aplicações              \n",
            " carga_horaria | 24                                                         \n",
            "-RECORD 7-------------------------------------------------------------------\n",
            " nome          | Lidando com BigData: Apache Spark, Hadoop, MapReduce, Hive \n",
            " carga_horaria | 24                                                         \n",
            "-RECORD 8-------------------------------------------------------------------\n",
            " nome          | Gerenciamento e Processamento de grande volume de dados    \n",
            " carga_horaria | 24                                                         \n",
            "-RECORD 9-------------------------------------------------------------------\n",
            " nome          | Internet das Coisas e Aplicações Distribuídas              \n",
            " carga_horaria | 24                                                         \n",
            "-RECORD 10------------------------------------------------------------------\n",
            " nome          | Deep Learning                                              \n",
            " carga_horaria | 24                                                         \n",
            "-RECORD 11------------------------------------------------------------------\n",
            " nome          | Business Intelligence e BigData                            \n",
            " carga_horaria | 24                                                         \n",
            "-RECORD 12------------------------------------------------------------------\n",
            " nome          | Atividades Integradoras                                    \n",
            " carga_horaria | 12                                                         \n",
            "-RECORD 13------------------------------------------------------------------\n",
            " nome          | Preparação para Projeto Aplicado                           \n",
            " carga_horaria | 36                                                         \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM6uzSooED1G",
        "outputId": "200d5c30-6353-4b60-9c41-7dd1aaca73fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Somente 5 registros\n",
        "df_especializacao.show(n=3, truncate=60)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------+-------------+\n",
            "|                              nome|carga_horaria|\n",
            "+----------------------------------+-------------+\n",
            "|  Introdução a BigData e Analytics|           36|\n",
            "|              Estatística aplicada|           24|\n",
            "|Visualização de dados e informação|           24|\n",
            "+----------------------------------+-------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UCGT1A7ED1I"
      },
      "source": [
        "#### Métodos `describe` e `summary`\n",
        "\n",
        "O método `describe` computa estatísticas descritivas básicas nas colunas numéricas e textuais. É utilizado em conjunto com o método `show` para exibição do resultado.\n",
        "\n",
        "**Atenção**: Esta operação pode ser bastante demorada em um DataFrame de maior volume. O motivo ficará claro ao longo da disciplina."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PusJ_HGnED1J",
        "outputId": "d726e582-c454-4a54-8c06-698e57a82b09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_especializacao.describe().show(truncate=60)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------------------+------------------+\n",
            "|summary|                              nome|     carga_horaria|\n",
            "+-------+----------------------------------+------------------+\n",
            "|  count|                                14|                14|\n",
            "|   mean|                              null|25.714285714285715|\n",
            "| stddev|                              null|6.4142698058981855|\n",
            "|    min|           Atividades Integradoras|                12|\n",
            "|    max|Visualização de dados e informação|                36|\n",
            "+-------+----------------------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp39KOiaED1K"
      },
      "source": [
        "Já o método `summary` computa algumas estatísticas a mais, os quantis. Sem informar parâmetros, summary irá calcular os quantis 25%, 50% (mediana) e 75%. O parâmetro de `summary` possiblita escolher quais estatísticas serão calculadas.\n",
        "\n",
        "As estatísticas disponíveis estão descritas na documentação do método ([link](http://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.summary)).\n",
        "\n",
        "**O mesmo alerta e tempo de processamento segue válido**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GogtwNPTED1L",
        "outputId": "44f2207e-8d5b-4e52-eb2f-5aefbd423880",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_especializacao.summary().show(truncate=60)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------------------+------------------+\n",
            "|summary|                              nome|     carga_horaria|\n",
            "+-------+----------------------------------+------------------+\n",
            "|  count|                                14|                14|\n",
            "|   mean|                              null|25.714285714285715|\n",
            "| stddev|                              null|6.4142698058981855|\n",
            "|    min|           Atividades Integradoras|                12|\n",
            "|    25%|                              null|                24|\n",
            "|    50%|                              null|                24|\n",
            "|    75%|                              null|                24|\n",
            "|    max|Visualização de dados e informação|                36|\n",
            "+-------+----------------------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDxZ81F0ED1M",
        "outputId": "52a17ecc-eb6d-4e4d-f4a4-5af7b9bdcf96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_especializacao.summary(\"count\", \"mean\", \"10%\", \"50%\", \"90%\").show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+------------------+\n",
            "|summary|nome|     carga_horaria|\n",
            "+-------+----+------------------+\n",
            "|  count|  14|                14|\n",
            "|   mean|null|25.714285714285715|\n",
            "|    10%|null|                24|\n",
            "|    50%|null|                24|\n",
            "|    90%|null|                36|\n",
            "+-------+----+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfgKaC32ED1N"
      },
      "source": [
        "#### Método `columns`\n",
        "\n",
        "Retorna uma lista com os nomes das colunas do DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVui1F3JED1O",
        "outputId": "1a1d7f2a-cf1f-4b1c-a920-5e769914031e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_especializacao.columns"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nome', 'carga_horaria']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM2-uKZjED1P"
      },
      "source": [
        "#### Método count\n",
        "\n",
        "Retorna a quantidade de registros de um DataFrame.\n",
        "\n",
        "**Atenção**: Por mais que não pareça intuitivo, este operação pode ser bastante demorada em um DataFrame de maior volume, e novamente digo que o motivo ficará claro ao longo da disciplina!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRgB-4-_ED1P",
        "outputId": "8e387682-dbcc-466b-9132-713d4a435781",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_especializacao.count()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE27_hcNED1R"
      },
      "source": [
        "## O caminho contrário\n",
        "\n",
        "Da mesma forma como conseguimos enviar dados do Python para o Spark (🐍➡️💥) podemos também trazer dados do Spark  para o Python (🐍⬅️💥).\n",
        "\n",
        "**Mas antes temos que conversar sobre volumes de dados.**\n",
        "\n",
        "> Neste Objeto de Aprendizagem estamos trabalhando com pequenos volumes de dados em ambiente local, então a transferência de dados não causará dores de cabeça. No entanto, considerem o cenário real de lidar com grandes volumes de dados em um cluster, em ordem de grandeza maior do que sua máquina é capaz de armazenar em memória. Pense em Terabytes (TB) de dados. Tentar transferir este volume de dados do cluster para sua máquina será um desastre.\n",
        "\n",
        "Na prática, a transferência de DataFrames do Spark para o Python é feita após algum processamento dos dados no Spark. Este processamento pode ser:\n",
        "- sumarização de dados (estatísticas descritivas, agrupamentos)\n",
        "- a seleção e filtro de um subconjunto de dados\n",
        "- amostragem\n",
        "- etc.\n",
        "\n",
        "E uma justificativa para transferências deste tipo é a necessidade de uso de recursos que não estão disponíveis no Spark. E mesmo assim temos formas de enviar recursos do Python para uso no Spark (faremos isso em outra oportunidade)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUdEJuoBED1S"
      },
      "source": [
        "#### Métodos `head`, `first` e `take`\n",
        "\n",
        "O método `head` retorna o **n** primeiros registros de um DataFrame, retornando somente 1 registro se o parâmetro **n** não for especificado.\n",
        "\n",
        "Uma pegadinha: Se não especificar o parâmetro, o objeto de retorno é o primeiro registro, de tipo `Row`. No entanto, se especificar **n=1** o retorno será de tipo `list` com o objeto `Row` dentro da lista. `head` sem parâmetros é equivalente ao método `first`.\n",
        "\n",
        "`take` é bastante similar a `head`, porém com parâmetro **num** obrigatório.\n",
        "\n",
        "Apesar da aparente confusão, pense que `head` é uma combinação de `first` e `take`:\n",
        "\n",
        "- `head` sem parâmetro equivale a `first`\n",
        "- `head` com parâmetro equivale a `take`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqyZJSJvED1T",
        "outputId": "19a34593-3ef6-4c8b-df1c-b27e0f7ab061",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "um = df_especializacao.head()\n",
        "lum = df_especializacao.head(n=5)\n",
        "\n",
        "print((um, type(um)))\n",
        "print((lum, type(lum)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Row(nome='Introdução a BigData e Analytics', carga_horaria=36), <class 'pyspark.sql.types.Row'>)\n",
            "([Row(nome='Introdução a BigData e Analytics', carga_horaria=36), Row(nome='Estatística aplicada', carga_horaria=24), Row(nome='Visualização de dados e informação', carga_horaria=24), Row(nome='Compartilhamento e segurança de dados', carga_horaria=24), Row(nome='Introdução a Python e linguagem R', carga_horaria=36)], <class 'list'>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0htVUAfED1U",
        "outputId": "75556c39-abe9-4fe4-886d-c51b085f9e49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_especializacao.first()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(nome='Introdução a BigData e Analytics', carga_horaria=36)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA1_FZznED1V",
        "outputId": "b8cb36ba-9d19-41ca-e955-bdea8eb31f2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_especializacao.take(num=14)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(nome='Introdução a BigData e Analytics', carga_horaria=36),\n",
              " Row(nome='Estatística aplicada', carga_horaria=24),\n",
              " Row(nome='Visualização de dados e informação', carga_horaria=24),\n",
              " Row(nome='Compartilhamento e segurança de dados', carga_horaria=24),\n",
              " Row(nome='Introdução a Python e linguagem R', carga_horaria=36),\n",
              " Row(nome='Machine Learning', carga_horaria=24),\n",
              " Row(nome='Processamento de Alto Desempenho e Aplicações', carga_horaria=24),\n",
              " Row(nome='Lidando com BigData: Apache Spark, Hadoop, MapReduce, Hive', carga_horaria=24),\n",
              " Row(nome='Gerenciamento e Processamento de grande volume de dados', carga_horaria=24),\n",
              " Row(nome='Internet das Coisas e Aplicações Distribuídas', carga_horaria=24),\n",
              " Row(nome='Deep Learning', carga_horaria=24),\n",
              " Row(nome='Business Intelligence e BigData', carga_horaria=24),\n",
              " Row(nome='Atividades Integradoras', carga_horaria=12),\n",
              " Row(nome='Preparação para Projeto Aplicado', carga_horaria=36)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EmfONjXED1W"
      },
      "source": [
        "#### Método `collect`\n",
        "\n",
        "Este método retorna **todos** os registros do DataFrame. \n",
        "\n",
        "**Cuidado** ao usar este método com grandes volumes de dados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpC0LcMOED1X",
        "outputId": "490dfdbb-d94e-4b0b-85e7-4f717164c22d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_especializacao.collect()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(nome='Introdução a BigData e Analytics', carga_horaria=36),\n",
              " Row(nome='Estatística aplicada', carga_horaria=24),\n",
              " Row(nome='Visualização de dados e informação', carga_horaria=24),\n",
              " Row(nome='Compartilhamento e segurança de dados', carga_horaria=24),\n",
              " Row(nome='Introdução a Python e linguagem R', carga_horaria=36),\n",
              " Row(nome='Machine Learning', carga_horaria=24),\n",
              " Row(nome='Processamento de Alto Desempenho e Aplicações', carga_horaria=24),\n",
              " Row(nome='Lidando com BigData: Apache Spark, Hadoop, MapReduce, Hive', carga_horaria=24),\n",
              " Row(nome='Gerenciamento e Processamento de grande volume de dados', carga_horaria=24),\n",
              " Row(nome='Internet das Coisas e Aplicações Distribuídas', carga_horaria=24),\n",
              " Row(nome='Deep Learning', carga_horaria=24),\n",
              " Row(nome='Business Intelligence e BigData', carga_horaria=24),\n",
              " Row(nome='Atividades Integradoras', carga_horaria=12),\n",
              " Row(nome='Preparação para Projeto Aplicado', carga_horaria=36)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld4mUs6NED1Y"
      },
      "source": [
        "## Finalizando a sessão\n",
        "\n",
        "Em muitos casos de uso o Cluster é um ambiente compartilhado e de recursos finitos. Ao concluir o uso de uma sessão do Spark sempre é recomendado finalizá-la para liberar os recursos alocados nesta sessão."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2NyUbK0ED1Z"
      },
      "source": [
        "spark.stop()"
      ],
      "execution_count": 28,
      "outputs": []
    }
  ]
}