{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "name": "primeiros_passos",
    "notebookId": 3892328798844911,
    "colab": {
      "name": "primeiros_passos.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3eqkIQOEDz6"
      },
      "source": [
        "# Primeiros passos com PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNnPdkuHED0O"
      },
      "source": [
        "Neste Objeto de Aprendizagem daremos nossos primeiros passos com o PySpark e Spark Dataframes. O objetivo aqui √© conhecer os principais objetos do PySpark e introduzir os m√©todos mais b√°sicos para familiarizar com a tecnologia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsLWL9bdED0S"
      },
      "source": [
        "Come√ßaremos pela importa√ß√£o do pacote do PySpark que engloba as opera√ß√µes com DataFrames e ent√£o criaremos um pequeno DataFrame que ser√° utilizado nos exemplos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR_Lm-dqED0U"
      },
      "source": [
        "### Bibliotecas necess√°rias\n",
        "\n",
        "Por enquanto precisaremos somente do m√≥dulo `pyspark.sql`. O pacote PySpark possui diversos m√≥dulos, mas por enquanto precisaremos somente dos objetos que est√£o em `pyspark.sql`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iikR-80tERzw"
      },
      "source": [
        "# instalar as depend√™ncias\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# configurar as vari√°veis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "# tornar o pyspark \"import√°vel\"\n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JITuELPPED0X"
      },
      "source": [
        "# Uso do Spark Dataframes no PySpark\n",
        "from pyspark.sql import *"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_ZFRiXFED0e"
      },
      "source": [
        "### Conectando com o Spark\n",
        "\n",
        "O pr√≥ximo passo √© iniciar uma sess√£o do Spark (`SparkSession`), cujo papel √© o de comunica√ß√£o com o Cluster. No exemplo abaixo criaremos uma sess√£o local - ou seja, com um minicluster na sua pr√≥pria m√°quina. Esta sess√£o local √© definida por meio do m√©todo `master`. O m√©todo `master` indica qual o tipo de Cluster onde conectaremos e outros detalhes. \n",
        "\n",
        "No nosso caso indicamos que o tipo de Cluster √© local e que este utilizaremos 2 processadores para execu√ß√£o das tarefas do Spark. Percebam que mesmo em modo local temos √† nossa disposi√ß√£o a capacidade de processamento _multicore_.\n",
        "\n",
        "**IMPORTANTE**: No ambiente da **Databricks** n√£o precisamos criar uma Sess√£o pois o notebook ser√° vinculado (_attached_) a um cluster no momento da execu√ß√£o. Logo, o bloco de c√≥digo abaixo n√£o ser√° necess√°rio **na Databricks**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcpRqsVtED0h"
      },
      "source": [
        "# Vamos trabalhar com o Spark localmente, sem o uso de um cluster.\n",
        "#spark = SparkSession \\\n",
        "#    .builder \\\n",
        "#    .master(\"local[2]\") \\\n",
        "#    .appName(\"Primeiros passos\") \\\n",
        "#    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4vGQ_0CED0k"
      },
      "source": [
        "## Cria√ß√£o de um Data Frame via c√≥digo\n",
        "\n",
        "Nesta sess√£o criaremos um DataFrame diretamente via c√≥digo. Esta n√£o √© uma pr√°tica muito comum, visto que trabalhamos com grandes volumes de dados obtidos atrav√©s de Sistemas de Armazenamento. \n",
        "\n",
        "Mas n√£o pensem que a cria√ß√£o de um DataFrame via c√≥digo s√≥ serve para exemplos! Ainda veremos casos onde esta pr√°tica ajudar√° na resolu√ß√£o de problemas!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kpD9HtRED0o"
      },
      "source": [
        "### Exemplo Pr√°tico\n",
        "\n",
        "Nosso exemplo pr√°tico utiliza a rela√ß√£o de disciplinas que comp√µe esta especializa√ß√£o e suas cargas hor√°rias!\n",
        "\n",
        "Os dados para este exemplo foram obtidos **manualmente** da p√°gina de Estrutura Curricular do curso, dispon√≠vel [aqui](http://www.unisinos.br/especializacao/big-data-data-science-e-data-analytics/ead/sao-leopoldo/estrutura-curricular)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RyUNAeiED0r"
      },
      "source": [
        "#### Defini√ß√£o da estrutura: Registros e Colunas\n",
        "\n",
        "Para criar um DataFrame por c√≥digo precisamos inicialmente definir sua estrutura. A linha de c√≥digo abaixo define que nosso DataFrame ser√° formado por disciplinas, onde cada registro (**Row**) ser√° uma disciplina. Os atributos de uma disciplina dispon√≠veis ser√£o o _nome_ e a _carga hor√°ria_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK2v7HsTED0u"
      },
      "source": [
        "# Estrutura do nosso DataFrame\n",
        "Disciplina = Row(\"nome\", \"carga_horaria\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi7eeyvaED0w"
      },
      "source": [
        "#### Cria√ß√£o de inst√¢ncias (registros e atributos)\n",
        "\n",
        "Nosso pr√≥ximo passo √© a cria√ß√£o de inst√¢ncias para popular o DataFrame. Usaremos a estrutura `Disciplina` rec√©m criada para instanciar cada uma das disciplinas da especializa√ß√£o e sua carga hor√°ria. \n",
        "\n",
        "Neste exemplo foi criada uma refer√™ncia (`d01` a `d14`) por disciplina para deixar o c√≥digo mais claro. No passo seguinte criaremos uma lista que agrupar√° todas as disciplinas e servir√° de fonte para envio dos dados ao Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnwawBmhED0y"
      },
      "source": [
        "# Cada uma das disciplinas da especializa√ß√£o √© criada como uma inst√¢ncia do registro Disciplina.\n",
        "\n",
        "d01 = Disciplina(\"Introdu√ß√£o a BigData e Analytics\", 36)\n",
        "d02 = Disciplina(\"Estat√≠stica aplicada\", 24)\n",
        "d03 = Disciplina(\"Visualiza√ß√£o de dados e informa√ß√£o\", 24)\n",
        "d04 = Disciplina(\"Compartilhamento e seguran√ßa de dados\", 24)\n",
        "d05 = Disciplina(\"Introdu√ß√£o a Python e linguagem R\", 36)\n",
        "d06 = Disciplina(\"Machine Learning\", 24)\n",
        "d07 = Disciplina(\"Processamento de Alto Desempenho e Aplica√ß√µes\", 24)\n",
        "d08 = Disciplina(\"Lidando com BigData: Apache Spark, Hadoop, MapReduce, Hive\", 24)\n",
        "d09 = Disciplina(\"Gerenciamento e Processamento de grande volume de dados\", 24)\n",
        "d10 = Disciplina(\"Internet das Coisas e Aplica√ß√µes Distribu√≠das\", 24)\n",
        "d11 = Disciplina(\"Deep Learning\", 24)\n",
        "d12 = Disciplina(\"Business Intelligence e BigData\", 24)\n",
        "d13 = Disciplina(\"Atividades Integradoras\", 12)\n",
        "d14 = Disciplina(\"Prepara√ß√£o para Projeto Aplicado\", 36)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYQ3u5fxED00"
      },
      "source": [
        "Por meio da fun√ß√£o `display` temos uma pr√©via do que ser√° nosso DataFrame! Atentem para o fato de que at√© aqui nossos dados est√£o no Python e n√£o no Spark. Ainda n√£o temos um DataFrame!\n",
        "\n",
        "No ambiente **Databricks** o resultado da fun√ß√£o `display` ser√° apresentado de forma mais amig√°vel pois o mecanismo de notebooks do ambiente est√° preparado para formata√ß√£o dos objetos Row do Spark. L√° a visualiza√ß√£o da lista `Row` e posteriormente do DataFrame ser√£o muito parecidas!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpm2mZ56ED01",
        "outputId": "869a42e7-0608-48f1-9c49-3f3c1b6d296f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "especializacao_bigdata_datascience = [d01, d02, d03, d04, d05, d06, d07, d08, d09, d10, d11, d12, d13, d14]\n",
        "\n",
        "display(especializacao_bigdata_datascience)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[Row(nome='Introdu√ß√£o a BigData e Analytics', carga_horaria=36),\n",
              " Row(nome='Estat√≠stica aplicada', carga_horaria=24),\n",
              " Row(nome='Visualiza√ß√£o de dados e informa√ß√£o', carga_horaria=24),\n",
              " Row(nome='Compartilhamento e seguran√ßa de dados', carga_horaria=24),\n",
              " Row(nome='Introdu√ß√£o a Python e linguagem R', carga_horaria=36),\n",
              " Row(nome='Machine Learning', carga_horaria=24),\n",
              " Row(nome='Processamento de Alto Desempenho e Aplica√ß√µes', carga_horaria=24),\n",
              " Row(nome='Lidando com BigData: Apache Spark, Hadoop, MapReduce, Hive', carga_horaria=24),\n",
              " Row(nome='Gerenciamento e Processamento de grande volume de dados', carga_horaria=24),\n",
              " Row(nome='Internet das Coisas e Aplica√ß√µes Distribu√≠das', carga_horaria=24),\n",
              " Row(nome='Deep Learning', carga_horaria=24),\n",
              " Row(nome='Business Intelligence e BigData', carga_horaria=24),\n",
              " Row(nome='Atividades Integradoras', carga_horaria=12),\n",
              " Row(nome='Prepara√ß√£o para Projeto Aplicado', carga_horaria=36)]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHarbS-RED03"
      },
      "source": [
        "#### Cria√ß√£o do DataFrame por meio da transfer√™ncia dos dados da lista\n",
        "\n",
        "Lembram que mais acima eu descrevi o `SparkSession` como o canal de comunica√ß√£o com o Cluster? Pois bem, agora veremos na pr√°tica o que isso significa. Nossa sess√£o possibilita a cria√ß√£o de um DataFrame pelo m√©todo `createDataFrame`. Este m√©todo:\n",
        "- envia a lista de objetos `Row` para o Cluster\n",
        "- cria uma estrutra de DataFrame no Cluster\n",
        "- popula o DataFrame com os objetos `Row` recebidos\n",
        "- retorna a refer√™ncia ao DataFrame para o Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS6sz0C1ED04"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "df_especializacao = spark.createDataFrame(especializacao_bigdata_datascience)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g75NP5ETED06"
      },
      "source": [
        "Voltaremos a usar a fun√ß√£o `display`, desta vez para inspecionar o conte√∫do da refer√™ncia ao DataFrame que o m√©todo `createDataFrame` retornou para n√≥s. Aqui percebemos que se trata de um DataFrame, e que ele possui duas colunas:\n",
        "\n",
        "- nome: string\n",
        "- carga_horaria: bigint\n",
        "\n",
        "**Importante**: Na **Databricks** a fun√ß√£o `display` exibe o conte√∫do do nosso DataFrame de forma bastante similar a quando usamos `display` para visualizar o conte√∫do da lista de objetos `Row`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3eNAFT4ED07",
        "outputId": "a37b85f2-8a0f-4efd-c739-daa4d6b957da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "display(df_especializacao)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[nome: string, carga_horaria: bigint]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ge-a1LTED08"
      },
      "source": [
        "### Visualiza√ß√£o de dados de um DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sof18u84ED09"
      },
      "source": [
        "#### M√©todo `show`\n",
        "\n",
        "O m√©todo `show` exibe registros do DataFrame formatados em modo texto. Se a chamada ao m√©todo for sem nenhum par√¢metro ele retornar√° uma tabela com os nomes das coluns em cabe√ßalho, registros at√© um m√°ximo de 20 linhas e os valores das colunas de tipo String (texto) ser√£o exibidos at√© um m√°ximo de 20 caracteres.\n",
        "\n",
        "A documenta√ß√£o do m√©todo `show` ([link](http://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show)) detalha os seguintes par√¢metros:\n",
        "\n",
        "- **n** ‚Äì N√∫mero de registros a exibir. Se quisermos uma quantidade diferente de 20 registros ent√£o devemos informar a quantidade neste par√¢metro.\n",
        "- **truncate** ‚Äì Se 20 caracteres for pouco (e no nosso exemplo vimos que √© pouco) ent√£o devemos informar quantos caracteres das colunas String devem ser mostrados. Se o DataFrame tiver muitas colunas do tipo String a visualiza√ß√£o pode ficar dif√≠cil.\n",
        "- **vertical** ‚Äì Se for False (padr√£o), exibe em formato de tabela. Se for True, exibir√° cada coluna em uma linha, em formato de lista de valores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egTiOFZ_ED0_",
        "outputId": "464b5c15-858c-42ed-de45-571bf48a69d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_especializacao.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------+\n",
            "|                nome|carga_horaria|\n",
            "+--------------------+-------------+\n",
            "|Introdu√ß√£o a BigD...|           36|\n",
            "|Estat√≠stica aplicada|           24|\n",
            "|Visualiza√ß√£o de d...|           24|\n",
            "|Compartilhamento ...|           24|\n",
            "|Introdu√ß√£o a Pyth...|           36|\n",
            "|    Machine Learning|           24|\n",
            "|Processamento de ...|           24|\n",
            "|Lidando com BigDa...|           24|\n",
            "|Gerenciamento e P...|           24|\n",
            "|Internet das Cois...|           24|\n",
            "|       Deep Learning|           24|\n",
            "|Business Intellig...|           24|\n",
            "|Atividades Integr...|           12|\n",
            "|Prepara√ß√£o para P...|           36|\n",
            "+--------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZEk7pToED1A",
        "outputId": "5ce454e4-22de-4991-f7ab-b85b337e5c0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Lista de registros, exibindo os primeiros 60 caracteres de cada nome.\n",
        "df_especializacao.show(vertical=True, truncate=60)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-RECORD 0-------------------------------------------------------------------\n",
            " nome          | Introdu√ß√£o a BigData e Analytics                           \n",
            " carga_horaria | 36                                                         \n",
            "-RECORD 1-------------------------------------------------------------------\n",
            " nome          | Estat√≠stica aplicada                                       \n",
            " carga_horaria | 24                                                         \n",
            "-RECORD 2-------------------------------------------------------------------\n",
            " nome          | Visualiza√ß√£o de dados e informa√ß√£o                         \n",
            " carga_horaria | 24                                                         \n",
            "-RECORD 3-------------------------------------------------------------------\n",
            " nome          | Compartilhamento e seguran√ßa de dados                      \n",
            " carga_horaria | 24                                                         \n",
            "-RECORD 4-------------------------------------------------------------------\n",
            " nome          | Introdu√ß√£o a Python e linguagem R                          \n",
            " carga_horaria | 36                                                         \n",
            "-RECORD 5-------------------------------------------------------------------\n",
            " nome          | Machine Learning                                           \n",
            " carga_horaria | 24                                                         \n",
            "-RECORD 6-------------------------------------------------------------------\n",
            " nome          | Processamento de Alto Desempenho e Aplica√ß√µes              \n",
            " carga_horaria | 24                                                         \n",
            "-RECORD 7-------------------------------------------------------------------\n",
            " nome          | Lidando com BigData: Apache Spark, Hadoop, MapReduce, Hive \n",
            " carga_horaria | 24                                                         \n",
            "-RECORD 8-------------------------------------------------------------------\n",
            " nome          | Gerenciamento e Processamento de grande volume de dados    \n",
            " carga_horaria | 24                                                         \n",
            "-RECORD 9-------------------------------------------------------------------\n",
            " nome          | Internet das Coisas e Aplica√ß√µes Distribu√≠das              \n",
            " carga_horaria | 24                                                         \n",
            "-RECORD 10------------------------------------------------------------------\n",
            " nome          | Deep Learning                                              \n",
            " carga_horaria | 24                                                         \n",
            "-RECORD 11------------------------------------------------------------------\n",
            " nome          | Business Intelligence e BigData                            \n",
            " carga_horaria | 24                                                         \n",
            "-RECORD 12------------------------------------------------------------------\n",
            " nome          | Atividades Integradoras                                    \n",
            " carga_horaria | 12                                                         \n",
            "-RECORD 13------------------------------------------------------------------\n",
            " nome          | Prepara√ß√£o para Projeto Aplicado                           \n",
            " carga_horaria | 36                                                         \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM6uzSooED1G",
        "outputId": "200d5c30-6353-4b60-9c41-7dd1aaca73fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Somente 5 registros\n",
        "df_especializacao.show(n=3, truncate=60)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------+-------------+\n",
            "|                              nome|carga_horaria|\n",
            "+----------------------------------+-------------+\n",
            "|  Introdu√ß√£o a BigData e Analytics|           36|\n",
            "|              Estat√≠stica aplicada|           24|\n",
            "|Visualiza√ß√£o de dados e informa√ß√£o|           24|\n",
            "+----------------------------------+-------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UCGT1A7ED1I"
      },
      "source": [
        "#### M√©todos `describe` e `summary`\n",
        "\n",
        "O m√©todo `describe` computa estat√≠sticas descritivas b√°sicas nas colunas num√©ricas e textuais. √â utilizado em conjunto com o m√©todo `show` para exibi√ß√£o do resultado.\n",
        "\n",
        "**Aten√ß√£o**: Esta opera√ß√£o pode ser bastante demorada em um DataFrame de maior volume. O motivo ficar√° claro ao longo da disciplina."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PusJ_HGnED1J",
        "outputId": "d726e582-c454-4a54-8c06-698e57a82b09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_especializacao.describe().show(truncate=60)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------------------+------------------+\n",
            "|summary|                              nome|     carga_horaria|\n",
            "+-------+----------------------------------+------------------+\n",
            "|  count|                                14|                14|\n",
            "|   mean|                              null|25.714285714285715|\n",
            "| stddev|                              null|6.4142698058981855|\n",
            "|    min|           Atividades Integradoras|                12|\n",
            "|    max|Visualiza√ß√£o de dados e informa√ß√£o|                36|\n",
            "+-------+----------------------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp39KOiaED1K"
      },
      "source": [
        "J√° o m√©todo `summary` computa algumas estat√≠sticas a mais, os quantis. Sem informar par√¢metros, summary ir√° calcular os quantis 25%, 50% (mediana) e 75%. O par√¢metro de `summary` possiblita escolher quais estat√≠sticas ser√£o calculadas.\n",
        "\n",
        "As estat√≠sticas dispon√≠veis est√£o descritas na documenta√ß√£o do m√©todo ([link](http://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.summary)).\n",
        "\n",
        "**O mesmo alerta e tempo de processamento segue v√°lido**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GogtwNPTED1L",
        "outputId": "44f2207e-8d5b-4e52-eb2f-5aefbd423880",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_especializacao.summary().show(truncate=60)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------------------+------------------+\n",
            "|summary|                              nome|     carga_horaria|\n",
            "+-------+----------------------------------+------------------+\n",
            "|  count|                                14|                14|\n",
            "|   mean|                              null|25.714285714285715|\n",
            "| stddev|                              null|6.4142698058981855|\n",
            "|    min|           Atividades Integradoras|                12|\n",
            "|    25%|                              null|                24|\n",
            "|    50%|                              null|                24|\n",
            "|    75%|                              null|                24|\n",
            "|    max|Visualiza√ß√£o de dados e informa√ß√£o|                36|\n",
            "+-------+----------------------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDxZ81F0ED1M",
        "outputId": "52a17ecc-eb6d-4e4d-f4a4-5af7b9bdcf96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_especializacao.summary(\"count\", \"mean\", \"10%\", \"50%\", \"90%\").show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+------------------+\n",
            "|summary|nome|     carga_horaria|\n",
            "+-------+----+------------------+\n",
            "|  count|  14|                14|\n",
            "|   mean|null|25.714285714285715|\n",
            "|    10%|null|                24|\n",
            "|    50%|null|                24|\n",
            "|    90%|null|                36|\n",
            "+-------+----+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfgKaC32ED1N"
      },
      "source": [
        "#### M√©todo `columns`\n",
        "\n",
        "Retorna uma lista com os nomes das colunas do DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVui1F3JED1O",
        "outputId": "1a1d7f2a-cf1f-4b1c-a920-5e769914031e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_especializacao.columns"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nome', 'carga_horaria']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM2-uKZjED1P"
      },
      "source": [
        "#### M√©todo count\n",
        "\n",
        "Retorna a quantidade de registros de um DataFrame.\n",
        "\n",
        "**Aten√ß√£o**: Por mais que n√£o pare√ßa intuitivo, este opera√ß√£o pode ser bastante demorada em um DataFrame de maior volume, e novamente digo que o motivo ficar√° claro ao longo da disciplina!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRgB-4-_ED1P",
        "outputId": "8e387682-dbcc-466b-9132-713d4a435781",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_especializacao.count()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE27_hcNED1R"
      },
      "source": [
        "## O caminho contr√°rio\n",
        "\n",
        "Da mesma forma como conseguimos enviar dados do Python para o Spark (üêç‚û°Ô∏èüí•) podemos tamb√©m trazer dados do Spark  para o Python (üêç‚¨ÖÔ∏èüí•).\n",
        "\n",
        "**Mas antes temos que conversar sobre volumes de dados.**\n",
        "\n",
        "> Neste Objeto de Aprendizagem estamos trabalhando com pequenos volumes de dados em ambiente local, ent√£o a transfer√™ncia de dados n√£o causar√° dores de cabe√ßa. No entanto, considerem o cen√°rio real de lidar com grandes volumes de dados em um cluster, em ordem de grandeza maior do que sua m√°quina √© capaz de armazenar em mem√≥ria. Pense em Terabytes (TB) de dados. Tentar transferir este volume de dados do cluster para sua m√°quina ser√° um desastre.\n",
        "\n",
        "Na pr√°tica, a transfer√™ncia de DataFrames do Spark para o Python √© feita ap√≥s algum processamento dos dados no Spark. Este processamento pode ser:\n",
        "- sumariza√ß√£o de dados (estat√≠sticas descritivas, agrupamentos)\n",
        "- a sele√ß√£o e filtro de um subconjunto de dados\n",
        "- amostragem\n",
        "- etc.\n",
        "\n",
        "E uma justificativa para transfer√™ncias deste tipo √© a necessidade de uso de recursos que n√£o est√£o dispon√≠veis no Spark. E mesmo assim temos formas de enviar recursos do Python para uso no Spark (faremos isso em outra oportunidade)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUdEJuoBED1S"
      },
      "source": [
        "#### M√©todos `head`, `first` e `take`\n",
        "\n",
        "O m√©todo `head` retorna o **n** primeiros registros de um DataFrame, retornando somente 1 registro se o par√¢metro **n** n√£o for especificado.\n",
        "\n",
        "Uma pegadinha: Se n√£o especificar o par√¢metro, o objeto de retorno √© o primeiro registro, de tipo `Row`. No entanto, se especificar **n=1** o retorno ser√° de tipo `list` com o objeto `Row` dentro da lista. `head` sem par√¢metros √© equivalente ao m√©todo `first`.\n",
        "\n",
        "`take` √© bastante similar a `head`, por√©m com par√¢metro **num** obrigat√≥rio.\n",
        "\n",
        "Apesar da aparente confus√£o, pense que `head` √© uma combina√ß√£o de `first` e `take`:\n",
        "\n",
        "- `head` sem par√¢metro equivale a `first`\n",
        "- `head` com par√¢metro equivale a `take`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqyZJSJvED1T",
        "outputId": "19a34593-3ef6-4c8b-df1c-b27e0f7ab061",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "um = df_especializacao.head()\n",
        "lum = df_especializacao.head(n=5)\n",
        "\n",
        "print((um, type(um)))\n",
        "print((lum, type(lum)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Row(nome='Introdu√ß√£o a BigData e Analytics', carga_horaria=36), <class 'pyspark.sql.types.Row'>)\n",
            "([Row(nome='Introdu√ß√£o a BigData e Analytics', carga_horaria=36), Row(nome='Estat√≠stica aplicada', carga_horaria=24), Row(nome='Visualiza√ß√£o de dados e informa√ß√£o', carga_horaria=24), Row(nome='Compartilhamento e seguran√ßa de dados', carga_horaria=24), Row(nome='Introdu√ß√£o a Python e linguagem R', carga_horaria=36)], <class 'list'>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0htVUAfED1U",
        "outputId": "75556c39-abe9-4fe4-886d-c51b085f9e49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_especializacao.first()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(nome='Introdu√ß√£o a BigData e Analytics', carga_horaria=36)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA1_FZznED1V",
        "outputId": "b8cb36ba-9d19-41ca-e955-bdea8eb31f2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_especializacao.take(num=14)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(nome='Introdu√ß√£o a BigData e Analytics', carga_horaria=36),\n",
              " Row(nome='Estat√≠stica aplicada', carga_horaria=24),\n",
              " Row(nome='Visualiza√ß√£o de dados e informa√ß√£o', carga_horaria=24),\n",
              " Row(nome='Compartilhamento e seguran√ßa de dados', carga_horaria=24),\n",
              " Row(nome='Introdu√ß√£o a Python e linguagem R', carga_horaria=36),\n",
              " Row(nome='Machine Learning', carga_horaria=24),\n",
              " Row(nome='Processamento de Alto Desempenho e Aplica√ß√µes', carga_horaria=24),\n",
              " Row(nome='Lidando com BigData: Apache Spark, Hadoop, MapReduce, Hive', carga_horaria=24),\n",
              " Row(nome='Gerenciamento e Processamento de grande volume de dados', carga_horaria=24),\n",
              " Row(nome='Internet das Coisas e Aplica√ß√µes Distribu√≠das', carga_horaria=24),\n",
              " Row(nome='Deep Learning', carga_horaria=24),\n",
              " Row(nome='Business Intelligence e BigData', carga_horaria=24),\n",
              " Row(nome='Atividades Integradoras', carga_horaria=12),\n",
              " Row(nome='Prepara√ß√£o para Projeto Aplicado', carga_horaria=36)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EmfONjXED1W"
      },
      "source": [
        "#### M√©todo `collect`\n",
        "\n",
        "Este m√©todo retorna **todos** os registros do DataFrame. \n",
        "\n",
        "**Cuidado** ao usar este m√©todo com grandes volumes de dados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpC0LcMOED1X",
        "outputId": "490dfdbb-d94e-4b0b-85e7-4f717164c22d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_especializacao.collect()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(nome='Introdu√ß√£o a BigData e Analytics', carga_horaria=36),\n",
              " Row(nome='Estat√≠stica aplicada', carga_horaria=24),\n",
              " Row(nome='Visualiza√ß√£o de dados e informa√ß√£o', carga_horaria=24),\n",
              " Row(nome='Compartilhamento e seguran√ßa de dados', carga_horaria=24),\n",
              " Row(nome='Introdu√ß√£o a Python e linguagem R', carga_horaria=36),\n",
              " Row(nome='Machine Learning', carga_horaria=24),\n",
              " Row(nome='Processamento de Alto Desempenho e Aplica√ß√µes', carga_horaria=24),\n",
              " Row(nome='Lidando com BigData: Apache Spark, Hadoop, MapReduce, Hive', carga_horaria=24),\n",
              " Row(nome='Gerenciamento e Processamento de grande volume de dados', carga_horaria=24),\n",
              " Row(nome='Internet das Coisas e Aplica√ß√µes Distribu√≠das', carga_horaria=24),\n",
              " Row(nome='Deep Learning', carga_horaria=24),\n",
              " Row(nome='Business Intelligence e BigData', carga_horaria=24),\n",
              " Row(nome='Atividades Integradoras', carga_horaria=12),\n",
              " Row(nome='Prepara√ß√£o para Projeto Aplicado', carga_horaria=36)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld4mUs6NED1Y"
      },
      "source": [
        "## Finalizando a sess√£o\n",
        "\n",
        "Em muitos casos de uso o Cluster √© um ambiente compartilhado e de recursos finitos. Ao concluir o uso de uma sess√£o do Spark sempre √© recomendado finaliz√°-la para liberar os recursos alocados nesta sess√£o."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2NyUbK0ED1Z"
      },
      "source": [
        "spark.stop()"
      ],
      "execution_count": 28,
      "outputs": []
    }
  ]
}