{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primeiros passos com PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste Objeto de Aprendizagem daremos nossos primeiros passos com o PySpark e Spark Dataframes. O objetivo aqui é conhecer os principais objetos do PySpark e introduzir os métodos mais básicos para familiarizar com a tecnologia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Começaremos pela importação do pacote do PySpark que engloba as operações com DataFrames e então criaremos um pequeno DataFrame que será utilizado nos exemplos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas necessárias\n",
    "\n",
    "Por enquanto precisaremos somente do módulo `pyspark.sql`. O pacote PySpark possui diversos módulos, mas por enquanto precisaremos somente dos objetos que estão em `pyspark.sql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-40b33c5464ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Uso do Spark Dataframes no PySpark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# Uso do Spark Dataframes no PySpark\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conectando com o Spark\n",
    "\n",
    "O próximo passo é iniciar uma sessão do Spark (`SparkSession`), cujo papel é o de comunicação com o Cluster. No exemplo abaixo criaremos uma sessão local - ou seja, com um minicluster na sua própria máquina. Esta sessão local é definida por meio do método `master`. O método `master` indica qual o tipo de Cluster onde conectaremos e outros detalhes. \n",
    "\n",
    "No nosso caso indicamos que o tipo de Cluster é local e que este utilizaremos 2 processadores para execução das tarefas do Spark. Percebam que mesmo em modo local temos à nossa disposição a capacidade de processamento _multicore_.\n",
    "\n",
    "**IMPORTANTE**: No ambiente da **Databricks** não precisamos criar uma Sessão pois o notebook será vinculado (_attached_) a um cluster no momento da execução. Logo, o bloco de código abaixo não será necessário **na Databricks**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos trabalhar com o Spark localmente, sem o uso de um cluster.\n",
    "#spark = SparkSession \\\n",
    "#    .builder \\\n",
    "#    .master(\"local[2]\") \\\n",
    "#    .appName(\"Primeiros passos\") \\\n",
    "#    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de um Data Frame via código\n",
    "\n",
    "Nesta sessão criaremos um DataFrame diretamente via código. Esta não é uma prática muito comum, visto que trabalhamos com grandes volumes de dados obtidos através de Sistemas de Armazenamento. \n",
    "\n",
    "Mas não pensem que a criação de um DataFrame via código só serve para exemplos! Ainda veremos casos onde esta prática ajudará na resolução de problemas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo Prático\n",
    "\n",
    "Nosso exemplo prático utiliza a relação de disciplinas que compõe esta especialização e suas cargas horárias!\n",
    "\n",
    "Os dados para este exemplo foram obtidos **manualmente** da página de Estrutura Curricular do curso, disponível [aqui](http://www.unisinos.br/especializacao/big-data-data-science-e-data-analytics/ead/sao-leopoldo/estrutura-curricular)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definição da estrutura: Registros e Colunas\n",
    "\n",
    "Para criar um DataFrame por código precisamos inicialmente definir sua estrutura. A linha de código abaixo define que nosso DataFrame será formado por disciplinas, onde cada registro (**Row**) será uma disciplina. Os atributos de uma disciplina disponíveis serão o _nome_ e a _carga horária_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Estrutura do nosso DataFrame\n",
    "Disciplina = Row(\"nome\", \"carga_horaria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criação de instâncias (registros e atributos)\n",
    "\n",
    "Nosso próximo passo é a criação de instâncias para popular o DataFrame. Usaremos a estrutura `Disciplina` recém criada para instanciar cada uma das disciplinas da especialização e sua carga horária. \n",
    "\n",
    "Neste exemplo foi criada uma referência (`d01` a `d14`) por disciplina para deixar o código mais claro. No passo seguinte criaremos uma lista que agrupará todas as disciplinas e servirá de fonte para envio dos dados ao Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cada uma das disciplinas da especialização é criada como uma instância do registro Disciplina.\n",
    "\n",
    "d01 = Disciplina(\"Introdução a BigData e Analytics\", 36)\n",
    "d02 = Disciplina(\"Estatística aplicada\", 24)\n",
    "d03 = Disciplina(\"Visualização de dados e informação\", 24)\n",
    "d04 = Disciplina(\"Compartilhamento e segurança de dados\", 24)\n",
    "d05 = Disciplina(\"Introdução a Python e linguagem R\", 36)\n",
    "d06 = Disciplina(\"Machine Learning\", 24)\n",
    "d07 = Disciplina(\"Processamento de Alto Desempenho e Aplicações\", 24)\n",
    "d08 = Disciplina(\"Lidando com BigData: Apache Spark, Hadoop, MapReduce, Hive\", 24)\n",
    "d09 = Disciplina(\"Gerenciamento e Processamento de grande volume de dados\", 24)\n",
    "d10 = Disciplina(\"Internet das Coisas e Aplicações Distribuídas\", 24)\n",
    "d11 = Disciplina(\"Deep Learning\", 24)\n",
    "d12 = Disciplina(\"Business Intelligence e BigData\", 24)\n",
    "d13 = Disciplina(\"Atividades Integradoras\", 12)\n",
    "d14 = Disciplina(\"Preparação para Projeto Aplicado\", 36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por meio da função `display` temos uma prévia do que será nosso DataFrame! Atentem para o fato de que até aqui nossos dados estão no Python e não no Spark. Ainda não temos um DataFrame!\n",
    "\n",
    "No ambiente **Databricks** o resultado da função `display` será apresentado de forma mais amigável pois o mecanismo de notebooks do ambiente está preparado para formatação dos objetos Row do Spark. Lá a visualização da lista `Row` e posteriormente do DataFrame serão muito parecidas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>nome</th><th>carga_horaria</th></tr></thead><tbody><tr><td>Introdução a BigData e Analytics</td><td>36</td></tr><tr><td>Estatística aplicada</td><td>24</td></tr><tr><td>Visualização de dados e informação</td><td>24</td></tr><tr><td>Compartilhamento e segurança de dados</td><td>24</td></tr><tr><td>Introdução a Python e linguagem R</td><td>36</td></tr><tr><td>Machine Learning</td><td>24</td></tr><tr><td>Processamento de Alto Desempenho e Aplicações</td><td>24</td></tr><tr><td>Lidando com BigData: Apache Spark, Hadoop, MapReduce, Hive</td><td>24</td></tr><tr><td>Gerenciamento e Processamento de grande volume de dados</td><td>24</td></tr><tr><td>Internet das Coisas e Aplicações Distribuídas</td><td>24</td></tr><tr><td>Deep Learning</td><td>24</td></tr><tr><td>Business Intelligence e BigData</td><td>24</td></tr><tr><td>Atividades Integradoras</td><td>12</td></tr><tr><td>Preparação para Projeto Aplicado</td><td>36</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "especializacao_bigdata_datascience = [d01, d02, d03, d04, d05, d06, d07, d08, d09, d10, d11, d12, d13, d14]\n",
    "\n",
    "display(especializacao_bigdata_datascience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criação do DataFrame por meio da transferência dos dados da lista\n",
    "\n",
    "Lembram que mais acima eu descrevi o `SparkSession` como o canal de comunicação com o Cluster? Pois bem, agora veremos na prática o que isso significa. Nossa sessão possibilita a criação de um DataFrame pelo método `createDataFrame`. Este método:\n",
    "- envia a lista de objetos `Row` para o Cluster\n",
    "- cria uma estrutra de DataFrame no Cluster\n",
    "- popula o DataFrame com os objetos `Row` recebidos\n",
    "- retorna a referência ao DataFrame para o Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_especializacao = spark.createDataFrame(especializacao_bigdata_datascience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voltaremos a usar a função `display`, desta vez para inspecionar o conteúdo da referência ao DataFrame que o método `createDataFrame` retornou para nós. Aqui percebemos que se trata de um DataFrame, e que ele possui duas colunas:\n",
    "\n",
    "- nome: string\n",
    "- carga_horaria: bigint\n",
    "\n",
    "**Importante**: Na **Databricks** a função `display` exibe o conteúdo do nosso DataFrame de forma bastante similar a quando usamos `display` para visualizar o conteúdo da lista de objetos `Row`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>nome</th><th>carga_horaria</th></tr></thead><tbody><tr><td>Introdução a BigData e Analytics</td><td>36</td></tr><tr><td>Estatística aplicada</td><td>24</td></tr><tr><td>Visualização de dados e informação</td><td>24</td></tr><tr><td>Compartilhamento e segurança de dados</td><td>24</td></tr><tr><td>Introdução a Python e linguagem R</td><td>36</td></tr><tr><td>Machine Learning</td><td>24</td></tr><tr><td>Processamento de Alto Desempenho e Aplicações</td><td>24</td></tr><tr><td>Lidando com BigData: Apache Spark, Hadoop, MapReduce, Hive</td><td>24</td></tr><tr><td>Gerenciamento e Processamento de grande volume de dados</td><td>24</td></tr><tr><td>Internet das Coisas e Aplicações Distribuídas</td><td>24</td></tr><tr><td>Deep Learning</td><td>24</td></tr><tr><td>Business Intelligence e BigData</td><td>24</td></tr><tr><td>Atividades Integradoras</td><td>12</td></tr><tr><td>Preparação para Projeto Aplicado</td><td>36</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_especializacao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização de dados de um DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método `show`\n",
    "\n",
    "O método `show` exibe registros do DataFrame formatados em modo texto. Se a chamada ao método for sem nenhum parâmetro ele retornará uma tabela com os nomes das coluns em cabeçalho, registros até um máximo de 20 linhas e os valores das colunas de tipo String (texto) serão exibidos até um máximo de 20 caracteres.\n",
    "\n",
    "A documentação do método `show` ([link](http://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show)) detalha os seguintes parâmetros:\n",
    "\n",
    "- **n** – Número de registros a exibir. Se quisermos uma quantidade diferente de 20 registros então devemos informar a quantidade neste parâmetro.\n",
    "- **truncate** – Se 20 caracteres for pouco (e no nosso exemplo vimos que é pouco) então devemos informar quantos caracteres das colunas String devem ser mostrados. Se o DataFrame tiver muitas colunas do tipo String a visualização pode ficar difícil.\n",
    "- **vertical** – Se for False (padrão), exibe em formato de tabela. Se for True, exibirá cada coluna em uma linha, em formato de lista de valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+--------------------+-------------+\n",
       "                nome|carga_horaria|\n",
       "+--------------------+-------------+\n",
       "Introdução a BigD...|           36|\n",
       "Estatística aplicada|           24|\n",
       "Visualização de d...|           24|\n",
       "Compartilhamento ...|           24|\n",
       "Introdução a Pyth...|           36|\n",
       "    Machine Learning|           24|\n",
       "Processamento de ...|           24|\n",
       "Lidando com BigDa...|           24|\n",
       "Gerenciamento e P...|           24|\n",
       "Internet das Cois...|           24|\n",
       "       Deep Learning|           24|\n",
       "Business Intellig...|           24|\n",
       "Atividades Integr...|           12|\n",
       "Preparação para P...|           36|\n",
       "+--------------------+-------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_especializacao.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">-RECORD 0-------------------------------------------------------------------\n",
       " nome          | Introdução a BigData e Analytics                           \n",
       " carga_horaria | 36                                                         \n",
       "-RECORD 1-------------------------------------------------------------------\n",
       " nome          | Estatística aplicada                                       \n",
       " carga_horaria | 24                                                         \n",
       "-RECORD 2-------------------------------------------------------------------\n",
       " nome          | Visualização de dados e informação                         \n",
       " carga_horaria | 24                                                         \n",
       "-RECORD 3-------------------------------------------------------------------\n",
       " nome          | Compartilhamento e segurança de dados                      \n",
       " carga_horaria | 24                                                         \n",
       "-RECORD 4-------------------------------------------------------------------\n",
       " nome          | Introdução a Python e linguagem R                          \n",
       " carga_horaria | 36                                                         \n",
       "-RECORD 5-------------------------------------------------------------------\n",
       " nome          | Machine Learning                                           \n",
       " carga_horaria | 24                                                         \n",
       "-RECORD 6-------------------------------------------------------------------\n",
       " nome          | Processamento de Alto Desempenho e Aplicações              \n",
       " carga_horaria | 24                                                         \n",
       "-RECORD 7-------------------------------------------------------------------\n",
       " nome          | Lidando com BigData: Apache Spark, Hadoop, MapReduce, Hive \n",
       " carga_horaria | 24                                                         \n",
       "-RECORD 8-------------------------------------------------------------------\n",
       " nome          | Gerenciamento e Processamento de grande volume de dados    \n",
       " carga_horaria | 24                                                         \n",
       "-RECORD 9-------------------------------------------------------------------\n",
       " nome          | Internet das Coisas e Aplicações Distribuídas              \n",
       " carga_horaria | 24                                                         \n",
       "-RECORD 10------------------------------------------------------------------\n",
       " nome          | Deep Learning                                              \n",
       " carga_horaria | 24                                                         \n",
       "-RECORD 11------------------------------------------------------------------\n",
       " nome          | Business Intelligence e BigData                            \n",
       " carga_horaria | 24                                                         \n",
       "-RECORD 12------------------------------------------------------------------\n",
       " nome          | Atividades Integradoras                                    \n",
       " carga_horaria | 12                                                         \n",
       "-RECORD 13------------------------------------------------------------------\n",
       " nome          | Preparação para Projeto Aplicado                           \n",
       " carga_horaria | 36                                                         \n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lista de registros, exibindo os primeiros 60 caracteres de cada nome.\n",
    "df_especializacao.show(vertical=True, truncate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----------------------------------+-------------+\n",
       "                              nome|carga_horaria|\n",
       "+----------------------------------+-------------+\n",
       "  Introdução a BigData e Analytics|           36|\n",
       "              Estatística aplicada|           24|\n",
       "Visualização de dados e informação|           24|\n",
       "+----------------------------------+-------------+\n",
       "only showing top 3 rows\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Somente 5 registros\n",
    "df_especializacao.show(n=3, truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métodos `describe` e `summary`\n",
    "\n",
    "O método `describe` computa estatísticas descritivas básicas nas colunas numéricas e textuais. É utilizado em conjunto com o método `show` para exibição do resultado.\n",
    "\n",
    "**Atenção**: Esta operação pode ser bastante demorada em um DataFrame de maior volume. O motivo ficará claro ao longo da disciplina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-------+----------------------------------+------------------+\n",
       "summary|                              nome|     carga_horaria|\n",
       "+-------+----------------------------------+------------------+\n",
       "  count|                                14|                14|\n",
       "   mean|                              null|25.714285714285715|\n",
       " stddev|                              null|6.4142698058981855|\n",
       "    min|           Atividades Integradoras|                12|\n",
       "    max|Visualização de dados e informação|                36|\n",
       "+-------+----------------------------------+------------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_especializacao.describe().show(truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já o método `summary` computa algumas estatísticas a mais, os quantis. Sem informar parâmetros, summary irá calcular os quantis 25%, 50% (mediana) e 75%. O parâmetro de `summary` possiblita escolher quais estatísticas serão calculadas.\n",
    "\n",
    "As estatísticas disponíveis estão descritas na documentação do método ([link](http://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.summary)).\n",
    "\n",
    "**O mesmo alerta e tempo de processamento segue válido**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-------+----------------------------------+------------------+\n",
       "summary|                              nome|     carga_horaria|\n",
       "+-------+----------------------------------+------------------+\n",
       "  count|                                14|                14|\n",
       "   mean|                              null|25.714285714285715|\n",
       " stddev|                              null|6.4142698058981855|\n",
       "    min|           Atividades Integradoras|                12|\n",
       "    25%|                              null|                24|\n",
       "    50%|                              null|                24|\n",
       "    75%|                              null|                24|\n",
       "    max|Visualização de dados e informação|                36|\n",
       "+-------+----------------------------------+------------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_especializacao.summary().show(truncate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-------+----+------------------+\n",
       "summary|nome|     carga_horaria|\n",
       "+-------+----+------------------+\n",
       "  count|  14|                14|\n",
       "   mean|null|25.714285714285715|\n",
       "    10%|null|                24|\n",
       "    50%|null|                24|\n",
       "    90%|null|                36|\n",
       "+-------+----+------------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_especializacao.summary(\"count\", \"mean\", \"10%\", \"50%\", \"90%\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método `columns`\n",
    "\n",
    "Retorna uma lista com os nomes das colunas do DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[13]: [&#39;nome&#39;, &#39;carga_horaria&#39;]</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_especializacao.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método count\n",
    "\n",
    "Retorna a quantidade de registros de um DataFrame.\n",
    "\n",
    "**Atenção**: Por mais que não pareça intuitivo, este operação pode ser bastante demorada em um DataFrame de maior volume, e novamente digo que o motivo ficará claro ao longo da disciplina!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[14]: 14</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_especializacao.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O caminho contrário\n",
    "\n",
    "Da mesma forma como conseguimos enviar dados do Python para o Spark (🐍➡️💥) podemos também trazer dados do Spark  para o Python (🐍⬅️💥).\n",
    "\n",
    "**Mas antes temos que conversar sobre volumes de dados.**\n",
    "\n",
    "> Neste Objeto de Aprendizagem estamos trabalhando com pequenos volumes de dados em ambiente local, então a transferência de dados não causará dores de cabeça. No entanto, considerem o cenário real de lidar com grandes volumes de dados em um cluster, em ordem de grandeza maior do que sua máquina é capaz de armazenar em memória. Pense em Terabytes (TB) de dados. Tentar transferir este volume de dados do cluster para sua máquina será um desastre.\n",
    "\n",
    "Na prática, a transferência de DataFrames do Spark para o Python é feita após algum processamento dos dados no Spark. Este processamento pode ser:\n",
    "- sumarização de dados (estatísticas descritivas, agrupamentos)\n",
    "- a seleção e filtro de um subconjunto de dados\n",
    "- amostragem\n",
    "- etc.\n",
    "\n",
    "E uma justificativa para transferências deste tipo é a necessidade de uso de recursos que não estão disponíveis no Spark. E mesmo assim temos formas de enviar recursos do Python para uso no Spark (faremos isso em outra oportunidade)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métodos `head`, `first` e `take`\n",
    "\n",
    "O método `head` retorna o **n** primeiros registros de um DataFrame, retornando somente 1 registro se o parâmetro **n** não for especificado.\n",
    "\n",
    "Uma pegadinha: Se não especificar o parâmetro, o objeto de retorno é o primeiro registro, de tipo `Row`. No entanto, se especificar **n=1** o retorno será de tipo `list` com o objeto `Row` dentro da lista. `head` sem parâmetros é equivalente ao método `first`.\n",
    "\n",
    "`take` é bastante similar a `head`, porém com parâmetro **num** obrigatório.\n",
    "\n",
    "Apesar da aparente confusão, pense que `head` é uma combinação de `first` e `take`:\n",
    "\n",
    "- `head` sem parâmetro equivale a `first`\n",
    "- `head` com parâmetro equivale a `take`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">(Row(nome=&#39;Introdução a BigData e Analytics&#39;, carga_horaria=36), &lt;class &#39;pyspark.sql.types.Row&#39;&gt;)\n",
       "([Row(nome=&#39;Introdução a BigData e Analytics&#39;, carga_horaria=36), Row(nome=&#39;Estatística aplicada&#39;, carga_horaria=24), Row(nome=&#39;Visualização de dados e informação&#39;, carga_horaria=24), Row(nome=&#39;Compartilhamento e segurança de dados&#39;, carga_horaria=24), Row(nome=&#39;Introdução a Python e linguagem R&#39;, carga_horaria=36)], &lt;class &#39;list&#39;&gt;)\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "um = df_especializacao.head()\n",
    "lum = df_especializacao.head(n=5)\n",
    "\n",
    "print((um, type(um)))\n",
    "print((lum, type(lum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[16]: Row(nome=&#39;Introdução a BigData e Analytics&#39;, carga_horaria=36)</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_especializacao.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[17]: [Row(nome=&#39;Introdução a BigData e Analytics&#39;, carga_horaria=36),\n",
       " Row(nome=&#39;Estatística aplicada&#39;, carga_horaria=24),\n",
       " Row(nome=&#39;Visualização de dados e informação&#39;, carga_horaria=24),\n",
       " Row(nome=&#39;Compartilhamento e segurança de dados&#39;, carga_horaria=24),\n",
       " Row(nome=&#39;Introdução a Python e linguagem R&#39;, carga_horaria=36),\n",
       " Row(nome=&#39;Machine Learning&#39;, carga_horaria=24),\n",
       " Row(nome=&#39;Processamento de Alto Desempenho e Aplicações&#39;, carga_horaria=24),\n",
       " Row(nome=&#39;Lidando com BigData: Apache Spark, Hadoop, MapReduce, Hive&#39;, carga_horaria=24),\n",
       " Row(nome=&#39;Gerenciamento e Processamento de grande volume de dados&#39;, carga_horaria=24),\n",
       " Row(nome=&#39;Internet das Coisas e Aplicações Distribuídas&#39;, carga_horaria=24),\n",
       " Row(nome=&#39;Deep Learning&#39;, carga_horaria=24),\n",
       " Row(nome=&#39;Business Intelligence e BigData&#39;, carga_horaria=24),\n",
       " Row(nome=&#39;Atividades Integradoras&#39;, carga_horaria=12),\n",
       " Row(nome=&#39;Preparação para Projeto Aplicado&#39;, carga_horaria=36)]</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_especializacao.take(num=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método `collect`\n",
    "\n",
    "Este método retorna **todos** os registros do DataFrame. \n",
    "\n",
    "**Cuidado** ao usar este método com grandes volumes de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[18]: [Row(nome=&#39;Introdução a BigData e Analytics&#39;, carga_horaria=36),\n",
       " Row(nome=&#39;Estatística aplicada&#39;, carga_horaria=24),\n",
       " Row(nome=&#39;Visualização de dados e informação&#39;, carga_horaria=24),\n",
       " Row(nome=&#39;Compartilhamento e segurança de dados&#39;, carga_horaria=24),\n",
       " Row(nome=&#39;Introdução a Python e linguagem R&#39;, carga_horaria=36),\n",
       " Row(nome=&#39;Machine Learning&#39;, carga_horaria=24),\n",
       " Row(nome=&#39;Processamento de Alto Desempenho e Aplicações&#39;, carga_horaria=24),\n",
       " Row(nome=&#39;Lidando com BigData: Apache Spark, Hadoop, MapReduce, Hive&#39;, carga_horaria=24),\n",
       " Row(nome=&#39;Gerenciamento e Processamento de grande volume de dados&#39;, carga_horaria=24),\n",
       " Row(nome=&#39;Internet das Coisas e Aplicações Distribuídas&#39;, carga_horaria=24),\n",
       " Row(nome=&#39;Deep Learning&#39;, carga_horaria=24),\n",
       " Row(nome=&#39;Business Intelligence e BigData&#39;, carga_horaria=24),\n",
       " Row(nome=&#39;Atividades Integradoras&#39;, carga_horaria=12),\n",
       " Row(nome=&#39;Preparação para Projeto Aplicado&#39;, carga_horaria=36)]</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_especializacao.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalizando a sessão\n",
    "\n",
    "Em muitos casos de uso o Cluster é um ambiente compartilhado e de recursos finitos. Ao concluir o uso de uma sessão do Spark sempre é recomendado finalizá-la para liberar os recursos alocados nesta sessão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "name": "primeiros_passos",
  "notebookId": 3892328798844911
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
